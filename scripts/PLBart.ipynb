{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9968cc08-6061-43d1-a253-8c2cd5f3b387",
   "metadata": {},
   "source": [
    "# Code Generation using PLBart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b030165-cec9-4701-a069-8434324b4299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt \n",
    "import torch\n",
    "from transformers import PLBartTokenizer, PLBartModel, RobertaTokenizer,PLBartForConditionalGeneration\n",
    "import sentencepiece as spm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645813e4-913b-4e22-bb49-102c603cebb4",
   "metadata": {},
   "source": [
    "### Set the language for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265f8708-a744-4bb5-b933-fe09c36c76bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"sql\"\n",
    "#language = \"py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3b03f8-dccf-4e0b-bfb5-8136b24a2973",
   "metadata": {},
   "source": [
    "### Check if GPU is avaialble for faster training/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59552b6b-d726-4e15-8544-cf632fbcc024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23efb3c5-214d-48f1-9cf8-751e038234a3",
   "metadata": {},
   "source": [
    "### Standardize Features and Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb526b0-56cd-438a-a972-00966591a709",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"Container for a single training/test features for an example.\"\"\"\n",
    "    def __init__(self,input_tokens,input_ids,target_tokens,target_ids):\n",
    "        self.input_tokens = input_tokens\n",
    "        self.input_ids = input_ids\n",
    "        self.target_tokens = target_tokens\n",
    "        self.target_ids = target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667ab87e-785b-4564-924c-88e8d1aa23d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(data,tokenizer):\n",
    "    '''Format each example through tokenization, padding and attaching start and end tokens'''\n",
    "    # Standardize Features\n",
    "    code = data['NL']\n",
    "    code_tokens=tokenizer.tokenize(code)[:20-2]\n",
    "    input_tokens =[tokenizer.bos_token]+code_tokens+[tokenizer.eos_token]\n",
    "    input_ids =  tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "    padding_length = 20 - len(input_ids)\n",
    "    input_ids+=[tokenizer.pad_token_id]*padding_length\n",
    "    \n",
    "    # Standardize Target\n",
    "    nat = data['Code']\n",
    "    nat_tokens=tokenizer.tokenize(nat)[:150-2]\n",
    "    target_tokens =[tokenizer.bos_token]+nat_tokens+[tokenizer.eos_token]\n",
    "    target_ids =  tokenizer.convert_tokens_to_ids(target_tokens)\n",
    "    padding_length = 150 - len(target_ids)\n",
    "    target_ids+=[tokenizer.pad_token_id]*padding_length\n",
    "    return InputFeatures(input_tokens,input_ids,target_tokens,target_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f01d328-4fce-4e56-8f49-047781f0076e",
   "metadata": {},
   "source": [
    "### Instantiate unclanlp's Pre-trained PLBart Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59baf3ba-ec75-4f97-abec-26f3d76e137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Features and Targets into tensors\n",
    "torch.tensor(self.examples[indx].input_ids),torch.tensor(self.examples[indx].target_ids)\n",
    "\n",
    "# Instantiate Token\n",
    "tokenizer = PLBartTokenizer.from_pretrained(\"uclanlp/plbart-base\")\n",
    "model = PLBartForConditionalGeneration.from_pretrained(\"uclanlp/plbart-base\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8243eb73-231d-4866-b090-fcff67918902",
   "metadata": {},
   "source": [
    "### Build training and testing Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a3710a-caf0-43a1-8145-3b382be5317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeData(Dataset):\n",
    "    '''Class to hold to hold instances of data'''\n",
    "    def __init__(self, tokenizer, dataset):\n",
    "        self.examples = []\n",
    "        for i in range(len(dataset)):\n",
    "          x = dataset.loc[i]\n",
    "          self.examples.append(convert_examples_to_features(x,tokenizer))\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, indx):       \n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4770c88b-1495-4ecb-96d2-1448a6db5fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV\n",
    "train_data = pd.read_csv(\"../StaQC_Data/{}_single_answer.train.csv\".format(language))\n",
    "val_data = pd.read_csv(\"../StaQC_Data/{}_single_answer.val.csv\".format(language))\n",
    "train_data = train_data[['Code','NL']]\n",
    "val_data = val_data[['Code','NL']]\n",
    "\n",
    "train_dataset = CodeData(tokenizer, train_data)\n",
    "val_dataset = CodeData(tokenizer,val_data)\n",
    "\n",
    "train_batch_size = 16 #batch size\n",
    "#gc.collect()\n",
    "torch.cuda.empty_cache() #empty cache\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                                  batch_size=train_batch_size,num_workers=1,shuffle=True) #initialize data loaders\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=train_batch_size,num_workers=1)\n",
    "num_train_epochs= 8 #number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f39f03a-840a-4714-bc57-594052e9d3a6",
   "metadata": {},
   "source": [
    "### Train and Validate the PLBart Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9807db53-7fa7-40af-9628-b62ad68a3960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr= 0.0001) #Adam optimizer\n",
    "\n",
    "train_loss_graph = [] #list to save training loss per epoch\n",
    "val_loss_graph = [] #list to save validation loss per epoch\n",
    "model.zero_grad()\n",
    "\n",
    "# Loop through dataloader for each epoch\n",
    "for idx in range(num_train_epochs): \n",
    "    #bar = tqdm(train_dataloader,total=len(train_dataloader))\n",
    "    #bar2 = tqdm(val_dataloader,total = len(val_dataloader))\n",
    "    tr_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    # Loop through all the batches for each epoch\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Seperate the features and targets from the batch\n",
    "        input_ids = batch[0].to(device) #input IDs\n",
    "        target_ids = batch[1].to(device) #targets\n",
    "        #source_mask = torch.where(input_ids!=tokenizer.pad_token_id, input_ids, torch.tensor(-100))\n",
    "        #target_mask = torch.where(target_ids!=tokenizer.pad_token_id, target_ids, torch.tensor(-100))\n",
    "\n",
    "        # Train the PLBart model\n",
    "        model.train()\n",
    "\n",
    "        # Retrieve the new outputs of the model for the current batch \n",
    "        model_out = model(input_ids=input_ids, labels=target_ids)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = model_out.loss\n",
    "        \n",
    "        # Calculate Gradients with Losss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Backpropogate the Gradients\n",
    "        optimizer.step()\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "\n",
    "    # Loop through the next batch in the validation data loader\n",
    "    for batch in val_dataloader:\n",
    "        model.eval()\n",
    "\n",
    "        # Seperate the feature and targets in this batch\n",
    "        input_ids = batch[0].to(device) #input IDs\n",
    "        target_ids = batch[1].to(device) #targets\n",
    "        #source_mask = input_ids.ne(sp.pad_id())\n",
    "        #target_mask = target_ids.ne(sp.pad_id())\n",
    "        \n",
    "        with torch.no_grad(): # Without changing gradients\n",
    "            # Cacluate the outputs for the current validation batch\n",
    "            model_out = model(input_ids=input_ids,labels=target_ids) #decoder_attention_mask=target_mask)\n",
    "           \n",
    "            # Calculate Validation loss\n",
    "            v_loss = model_out.loss\n",
    "            val_loss +=v_loss.item()\n",
    "\n",
    "    # Calculate the average Loss over the batch for both training and validation\n",
    "    epoch_loss = tr_loss/len(train_dataloader) #training loss per epoch\n",
    "    epoch_val_loss = val_loss/len(val_dataloader) #validation loss per epoch\n",
    "    print(\"epoch {} train loss {} val loss {}\".format(idx+1,epoch_loss,epoch_val_loss))\n",
    "    train_loss_graph.append(epoch_loss) \n",
    "    val_loss_graph.append(epoch_val_loss)\n",
    "    torch.save(model.state_dict(), 'PLBART_{}_model-{}.pkl'.format(language, idx+1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b764ea9-25de-4023-8cf1-f6a2239705fb",
   "metadata": {},
   "source": [
    "### Plot Training and Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12efbf9d-0b85-426c-96f6-ded10c892f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_graph,'k')\n",
    "plt.plot(val_loss_graph,'y')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train Loss','Val Loss'])\n",
    "plt.title('Loss vs Epoch')\n",
    "plt.savefig('Loss-Plot-{}-PLBART.png'.format(language.upper()))\n",
    "\n",
    "d = pd.DataFrame({'Train_Loss':train_loss_graph,'Val_Loss':val_loss_graph})\n",
    "d.to_csv('Training_loss_{}_PLBART.csv'.format(language.upper()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
